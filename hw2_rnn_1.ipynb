{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import ngrams\n",
    "import string\n",
    "import re\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = []\n",
    "with open('snli_val.tsv','r', encoding = 'utf-8', newline = '') as file:\n",
    "    tsvreader = csv.reader(file, delimiter='\\t')\n",
    "    data_val = [line for line in tsvreader]\n",
    "    \n",
    "data_train = []\n",
    "with open('snli_train.tsv','r', encoding = 'utf-8', newline = '') as file:\n",
    "    tsvreader = csv.reader(file, delimiter='\\t')\n",
    "    data_train = [line for line in tsvreader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100001\n",
      "Validation dataset size is 1001\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size is {}\".format(len(data_train)))\n",
    "print(\"Validation dataset size is {}\".format(len(data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_s1, data_val_s2, data_val_label = zip(*data_val)\n",
    "data_train_s1, data_train_s2, data_train_label = zip(*data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_s1 = data_val_s1[1:]\n",
    "data_val_s2 = data_val_s2[1:]\n",
    "data_val_label = data_val_label[1:]\n",
    "data_train_s1 = data_train_s1[1:]\n",
    "data_train_s2 = data_train_s2[1:]\n",
    "data_train_label = data_train_label[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize data\n",
    "\"\"\"\n",
    "def tokenize_ngrams(sent):\n",
    "    #tokens = ngrams(re.findall(r\"[\\w']+|[.,!?;():~@+-<>#]\", sent.lower()),n)\n",
    "    tokens = ngrams([gram for gram in re.findall(r\"[\\w']+|[.,!?;():~@+-<>#]*\", sent.lower())\n",
    "                     if (gram not in string.punctuation)], 1)\n",
    "    return [token[0] for token in tokens]\n",
    "\n",
    "def tokenize_dataset_ngrams(dataset):\n",
    "    all_tokens = []\n",
    "    token_dataset = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize_ngrams(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "    return token_dataset, all_tokens\n",
    "    \n",
    "def convert_labels(dataset_label):\n",
    "    \"\"\"Convert labels from number\n",
    "       entails  --> 1\n",
    "       contradiction --> -1\n",
    "       neural --> 0\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for l in dataset_label:\n",
    "        if l == 'entailment':\n",
    "            labels.append(2)\n",
    "        elif l == 'contradiction':\n",
    "            labels.append(0)\n",
    "        elif l == 'neutral':\n",
    "            labels.append(1)\n",
    "        elif l=='label':\n",
    "            labels.append('label')\n",
    "    return labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_s1_tokens, _ = tokenize_dataset_ngrams(data_val_s1)\n",
    "data_val_s2_tokens, _ = tokenize_dataset_ngrams(data_val_s2)\n",
    "data_val_label0 = convert_labels(data_val_label)\n",
    "data_train_s1_tokens, all_train_s1_tokens = tokenize_dataset_ngrams(data_train_s1)\n",
    "data_train_s2_tokens, all_train_s2_tokens = tokenize_dataset_ngrams(data_train_s2)\n",
    "data_train_label0 = convert_labels(data_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of all tokens in train dataset is 2037612\n"
     ]
    }
   ],
   "source": [
    "all_train_tokens = all_train_s1_tokens + all_train_s2_tokens\n",
    "print(\"Total number of all tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(data_val_label0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_tokens = list(zip(data_val_s1_tokens, data_val_s2_tokens, data_val_label0))\n",
    "data_train_tokens = list(zip(data_train_s1_tokens, data_train_s2_tokens, data_train_label0))\n",
    "pkl.dump(data_val_tokens, open(\"data_val_tokens.p\", \"wb\"))\n",
    "pkl.dump(data_train_tokens, open(\"data_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_train_tokens.p   hw2_rnn_1-Copy1.ipynb  pre_emb_matrix\r\n",
      "data_train_tokens.p  hw2_rnn_1.ipynb\t    result\r\n",
      "data_val_tokens.p    hw2_rnn_2.ipynb\t    snli_train.tsv\r\n",
      "hw2_cnn-Copy1.ipynb  mnli_train.tsv\t    snli_val.tsv\r\n",
      "hw2_cnn.ipynb\t     mnli_val.tsv\t    wiki-news-300d-1M.vec\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build Vocabulary\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 20000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size-2))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(token2id)< max_vocab_size:\n",
    "    max_vocab_size = len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19581\n"
     ]
    }
   ],
   "source": [
    "print(len(token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_matrix shape is (200000, 300)\n",
      "Embedding Vocabualry size is 200000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load pretrained embedding matrix\"\"\"\n",
    "\n",
    "words_to_load = 200000\n",
    "\n",
    "with open('wiki-news-300d-1M.vec', 'r', encoding = 'utf-8') as f:\n",
    "    embedding_matrix = np.zeros([words_to_load, 300])\n",
    "    word2id_emb = {}\n",
    "    id2word_emb = []\n",
    "    next(f)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        id2word_emb.append(s[0])\n",
    "        word2id_emb[s[0]] = 0\n",
    "        embedding_matrix[i] = np.asarray(s[1:])\n",
    "\n",
    "print(\"Embedding_matrix shape is {}\".format(embedding_matrix.shape))\n",
    "print(\"Embedding Vocabualry size is {}\".format(len(id2word_emb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282008069046525 tokens appear in the pretrained dataset\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build pretrained Embedding Matrix\"\"\"\n",
    "pre_emb_matrix = np.zeros([max_vocab_size, 300])\n",
    "\n",
    "i = 0\n",
    "for word in id2token:\n",
    "    try:\n",
    "        id_pretrain = word2id_emb[word]\n",
    "        pre_emb_matrix[token2id[word]] = embedding_matrix[id_pretrain]\n",
    "    except KeyError:\n",
    "        pre_emb_matrix[token2id[word]] = np.zeros(300)\n",
    "        i = i+1\n",
    "print(\"{} tokens appear in the pretrained dataset\".format(1-i/max_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19581, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(token2id, open(\"token2id.p\", \"wb\"))\n",
    "pkl.dump(id2token, open(\"id2token.p\", \"wb\"))\n",
    "pkl.dump(pre_emb_matrix, open(\"pre_emb_matrix\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_s1_indices = token2index_dataset(data_val_s1_tokens, token2id)\n",
    "data_val_s2_indices = token2index_dataset(data_val_s2_tokens, token2id)\n",
    "data_train_s1_indices = token2index_dataset(data_train_s1_tokens, token2id)\n",
    "data_train_s2_indices = token2index_dataset(data_train_s2_tokens, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100000, 100000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_val_s1_indices), len(data_train_s2_indices), len(data_train_label0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list_s1, data_list_s2, data_label_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.s1_list, self.s2_list, self.label_list = data_list_s1, data_list_s2, data_label_list\n",
    "        assert ((len(self.s1_list) == len(self.label_list)) and (len(self.s1_list) == len(self.s2_list)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        return [self.s1_list[key], self.s2_list[key], max(len(self.s1_list[key]), len(self.s2_list[key])), self.label_list[key]]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list = []\n",
    "    s2_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[3])\n",
    "        length_list.append(datum[2])\n",
    "    # padding\n",
    "    #MAX_WORD_LENGTH\n",
    "    max_length = max(length_list)\n",
    "    for datum in batch:\n",
    "        padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,max_length-len(datum[0]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,max_length-len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1_list.append(padded_vec_s1)\n",
    "        s2_list.append(padded_vec_s2)\n",
    "    ind_dec_order = np.argsort(length_list)[::-1]\n",
    "    s1_list = np.array(s1_list)[ind_dec_order]\n",
    "    s2_list = np.array(s2_list)[ind_dec_order]\n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(s1_list)), torch.from_numpy(np.array(s2_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset(data_train_s1_indices, data_train_s2_indices, data_train_label0)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(data_val_s1_indices, data_val_s2_indices, data_val_label0)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of s1 32\n",
      "length of s2 32\n",
      "tensor([0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 0, 2, 0, 1, 1, 2, 2,\n",
      "        2, 2, 1, 0, 2, 0, 2, 0])\n",
      "length of s1 32\n",
      "length of s2 32\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 1, 2, 1, 2, 1, 0, 1, 2, 2, 0,\n",
      "        2, 1, 2, 2, 0, 0, 0, 0])\n",
      "length of s1 32\n",
      "length of s2 32\n",
      "tensor([2, 1, 0, 2, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 2, 2, 1, 2, 0, 0, 0])\n",
      "length of s1 32\n",
      "length of s2 32\n",
      "tensor([2, 2, 2, 0, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2, 1, 0, 2,\n",
      "        1, 0, 0, 0, 0, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for s1, s2, length, label in train_loader:\n",
    "    print('length of s1', len(s1))\n",
    "    print('length of s2', len(s2))\n",
    "    print(label)\n",
    "    n = 1+n\n",
    "    if n>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 21, 23, 4, 2, 101, 20, 30, 7, 2, 720, 1934, 2, 351, 10, 50],\n",
       " [2, 21, 23, 114, 3, 807, 78, 3, 50],\n",
       " 16,\n",
       " 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build RNN model\n",
    "\n",
    "bi-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = nn.RNN(10, 20,2)\n",
    "input = torch.randn(5,3,10)\n",
    "h0 = torch.randn(2,3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2706,  0.0511, -0.2846,  1.3676,  0.0936,  0.8250, -1.6942,  0.0878,\n",
       "         -4.1304, -1.5029],\n",
       "        [ 0.4704, -0.9433, -0.0144, -0.3048, -0.1974,  1.0391,  1.0443,  1.4158,\n",
       "          2.2376, -2.0218],\n",
       "        [-1.4847, -0.1551,  1.5994,  0.0388,  0.7863, -0.0203, -0.1791, -0.4830,\n",
       "         -0.7885,  1.0997],\n",
       "        [ 1.6072,  0.6304,  0.4567,  0.8326,  0.2975,  1.0990,  0.4029,  0.6755,\n",
       "         -0.2011, -0.8112],\n",
       "        [-0.6082, -0.5086, -0.0764,  0.6497, -0.4813, -0.2656, -0.3421,  0.1383,\n",
       "          1.7226, -0.1734]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 20]), torch.Size([2, 3, 20]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, h1 = rnn(input, h0)\n",
    "output.shape, h1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 30])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((input, output), dim =-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, pretrained_weight):\n",
    "        \"\"\"\n",
    "        GRU accepts the following hyperparams:\n",
    "        input_size - The number of expected features of in the input x\n",
    "        hidden_size - The number of features in the hidden state h\n",
    "        num_layers - Here we use the Default:1\n",
    "        bias - True\n",
    "        batch_first - The input and output tensors are provided as (batch, seq, feature)\n",
    "        bidirectional - \n",
    "        ===================================================================================\n",
    "        Note: padding_idx = 0\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        layer_size2 = 20\n",
    "        #layer_size2 = 200\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx = 0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional = True) # The first dimension is the batch dimension\n",
    "        self.linear1 = nn.Linear(4*hidden_size, layer_size2)\n",
    "        self.linear2 = nn.Linear(layer_size2, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden.to(device)\n",
    "\n",
    "    def forward(self, s1, s2, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = s1.size()\n",
    "\n",
    "        self.hidden_s1 = self.init_hidden(batch_size)\n",
    "        self.hidden_s2 = self.init_hidden(batch_size)\n",
    "        \n",
    "        embed_s1 = self.embedding(s1)\n",
    "        embed_s2 = self.embedding(s2)\n",
    "    \n",
    "        rnn_out_s1, self.hidden_s1 = self.gru(embed_s1, self.hidden_s1)\n",
    "        rnn_out_s2, self.hidden_s2 = self.gru(embed_s2, self.hidden_s2)\n",
    "        \n",
    "        rnn_out_s1 = torch.sum(rnn_out_s1, dim=1)\n",
    "        rnn_out_s2 = torch.sum(rnn_out_s2, dim=1)\n",
    "        \n",
    "        rnn_out = torch.cat((rnn_out_s1, rnn_out_s2),-1)\n",
    "        \n",
    "        rnn_out = self.linear1(rnn_out)\n",
    "        rnn_out = F.relu(rnn_out)\n",
    "        logits = self.linear2(rnn_out)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for s1, s2, lengths, labels in loader:\n",
    "        s1_batch, s2_batch, lengths_batch, label_batch = s1.to(device), s2.to(device), lengths.to(device), labels.to(device)\n",
    "        outputs = F.softmax(model(s1_batch, s2_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label_batch.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model2(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    for s1, s2, lengths, labels in loader:\n",
    "        s1_batch, s2_batch, lengths_batch, label_batch = s1.to(device), s2.to(device), lengths.to(device), labels.to(device)\n",
    "        probability = model(s1_batch, s2_batch, lengths_batch)\n",
    "        loss_test = float(criterion_test(probability, label_batch))\n",
    "        total_loss += loss_test\n",
    "        outputs = F.softmax(probability, dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += label_batch.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), total_loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8ae49d6eca62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0macc0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "acc0 = test_model(val_loader, model)\n",
    "acc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1, loss1 = test_model2(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pre_emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19581, (19581, 300))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2token), pre_emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('result/rnn_hs800.p', 'result/rnn_acc_hs800.pdf', 'result/model_rnn_hs800.pt')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 800\n",
    "result_FILE = 'result/rnn_hs'+str(HIDDEN_SIZE) +'.p'\n",
    "figname = 'result/rnn_acc_hs'+str(HIDDEN_SIZE)  +'.pdf'\n",
    "model_name = 'result/model_rnn_hs'+str(HIDDEN_SIZE) +'.pt'\n",
    "result_FILE, figname, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(emb_size=300, hidden_size=HIDDEN_SIZE, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token), pretrained_weight = pre_emb_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11227983\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_FILE = 'result/rnn_hs100.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [301/3125], Val_Acc: 42.0\n",
      "Epoch: [1/10], Step: [601/3125], Val_Acc: 46.5\n",
      "Epoch: [1/10], Step: [901/3125], Val_Acc: 47.6\n",
      "Epoch: [1/10], Step: [1201/3125], Val_Acc: 51.6\n",
      "Epoch: [1/10], Step: [1501/3125], Val_Acc: 50.1\n",
      "Epoch: [1/10], Step: [1801/3125], Val_Acc: 50.1\n",
      "Epoch: [1/10], Step: [2101/3125], Val_Acc: 51.2\n",
      "Epoch: [1/10], Step: [2401/3125], Val_Acc: 51.2\n",
      "Epoch: [1/10], Step: [2701/3125], Val_Acc: 54.8\n",
      "Epoch: [1/10], Step: [3001/3125], Val_Acc: 55.7\n",
      "Epoch: [2/10], Step: [301/3125], Val_Acc: 57.8\n",
      "Epoch: [2/10], Step: [601/3125], Val_Acc: 60.4\n",
      "Epoch: [2/10], Step: [901/3125], Val_Acc: 61.3\n",
      "Epoch: [2/10], Step: [1201/3125], Val_Acc: 63.3\n",
      "Epoch: [2/10], Step: [1501/3125], Val_Acc: 61.2\n",
      "Epoch: [2/10], Step: [1801/3125], Val_Acc: 62.5\n",
      "Epoch: [2/10], Step: [2101/3125], Val_Acc: 62.8\n",
      "Epoch: [2/10], Step: [2401/3125], Val_Acc: 62.8\n",
      "Epoch: [2/10], Step: [2701/3125], Val_Acc: 63.2\n",
      "Epoch: [2/10], Step: [3001/3125], Val_Acc: 62.9\n",
      "Epoch: [3/10], Step: [301/3125], Val_Acc: 62.8\n",
      "Epoch: [3/10], Step: [601/3125], Val_Acc: 65.2\n",
      "Epoch: [3/10], Step: [901/3125], Val_Acc: 65.7\n",
      "Epoch: [3/10], Step: [1201/3125], Val_Acc: 64.9\n",
      "Epoch: [3/10], Step: [1501/3125], Val_Acc: 63.2\n",
      "Epoch: [3/10], Step: [1801/3125], Val_Acc: 63.9\n",
      "Epoch: [3/10], Step: [2101/3125], Val_Acc: 65.5\n",
      "Epoch: [3/10], Step: [2401/3125], Val_Acc: 64.3\n",
      "Epoch: [3/10], Step: [2701/3125], Val_Acc: 63.6\n",
      "Epoch: [3/10], Step: [3001/3125], Val_Acc: 65.3\n",
      "Epoch: [4/10], Step: [301/3125], Val_Acc: 65.3\n",
      "Epoch: [4/10], Step: [601/3125], Val_Acc: 67.4\n",
      "Epoch: [4/10], Step: [901/3125], Val_Acc: 66.4\n",
      "Epoch: [4/10], Step: [1201/3125], Val_Acc: 66.6\n",
      "Epoch: [4/10], Step: [1501/3125], Val_Acc: 66.6\n",
      "Epoch: [4/10], Step: [1801/3125], Val_Acc: 68.3\n",
      "Epoch: [4/10], Step: [2101/3125], Val_Acc: 66.4\n",
      "Epoch: [4/10], Step: [2401/3125], Val_Acc: 66.3\n",
      "Epoch: [4/10], Step: [2701/3125], Val_Acc: 66.0\n",
      "Epoch: [4/10], Step: [3001/3125], Val_Acc: 66.8\n",
      "Epoch: [5/10], Step: [301/3125], Val_Acc: 67.4\n",
      "Epoch: [5/10], Step: [601/3125], Val_Acc: 67.1\n",
      "Epoch: [5/10], Step: [901/3125], Val_Acc: 68.0\n",
      "Epoch: [5/10], Step: [1201/3125], Val_Acc: 67.0\n",
      "Epoch: [5/10], Step: [1501/3125], Val_Acc: 66.7\n",
      "Epoch: [5/10], Step: [1801/3125], Val_Acc: 67.2\n",
      "Epoch: [5/10], Step: [2101/3125], Val_Acc: 67.2\n",
      "Epoch: [5/10], Step: [2401/3125], Val_Acc: 68.3\n",
      "Epoch: [5/10], Step: [2701/3125], Val_Acc: 68.7\n",
      "Epoch: [5/10], Step: [3001/3125], Val_Acc: 66.9\n",
      "Epoch: [6/10], Step: [301/3125], Val_Acc: 65.8\n",
      "Epoch: [6/10], Step: [601/3125], Val_Acc: 68.1\n",
      "Epoch: [6/10], Step: [901/3125], Val_Acc: 66.1\n",
      "Epoch: [6/10], Step: [1201/3125], Val_Acc: 65.2\n",
      "Epoch: [6/10], Step: [1501/3125], Val_Acc: 67.5\n",
      "Epoch: [6/10], Step: [1801/3125], Val_Acc: 66.2\n",
      "Epoch: [6/10], Step: [2101/3125], Val_Acc: 66.8\n",
      "Epoch: [6/10], Step: [2401/3125], Val_Acc: 67.8\n",
      "Epoch: [6/10], Step: [2701/3125], Val_Acc: 67.0\n",
      "Epoch: [6/10], Step: [3001/3125], Val_Acc: 65.9\n",
      "Epoch: [7/10], Step: [301/3125], Val_Acc: 66.3\n",
      "Epoch: [7/10], Step: [601/3125], Val_Acc: 66.1\n",
      "Epoch: [7/10], Step: [901/3125], Val_Acc: 67.2\n",
      "Epoch: [7/10], Step: [1201/3125], Val_Acc: 65.4\n",
      "Epoch: [7/10], Step: [1501/3125], Val_Acc: 67.1\n",
      "Epoch: [7/10], Step: [1801/3125], Val_Acc: 66.8\n",
      "Epoch: [7/10], Step: [2101/3125], Val_Acc: 67.9\n",
      "Epoch: [7/10], Step: [2401/3125], Val_Acc: 67.5\n",
      "Epoch: [7/10], Step: [2701/3125], Val_Acc: 67.2\n",
      "Epoch: [7/10], Step: [3001/3125], Val_Acc: 66.1\n",
      "Epoch: [8/10], Step: [301/3125], Val_Acc: 67.9\n",
      "Epoch: [8/10], Step: [601/3125], Val_Acc: 67.5\n",
      "Epoch: [8/10], Step: [901/3125], Val_Acc: 66.4\n",
      "Epoch: [8/10], Step: [1201/3125], Val_Acc: 66.3\n",
      "Epoch: [8/10], Step: [1501/3125], Val_Acc: 66.6\n",
      "Epoch: [8/10], Step: [1801/3125], Val_Acc: 66.3\n",
      "Epoch: [8/10], Step: [2101/3125], Val_Acc: 65.6\n",
      "Epoch: [8/10], Step: [2401/3125], Val_Acc: 66.1\n",
      "Epoch: [8/10], Step: [2701/3125], Val_Acc: 66.3\n",
      "Epoch: [8/10], Step: [3001/3125], Val_Acc: 66.0\n",
      "Epoch: [9/10], Step: [301/3125], Val_Acc: 66.5\n",
      "Epoch: [9/10], Step: [601/3125], Val_Acc: 66.5\n",
      "Epoch: [9/10], Step: [901/3125], Val_Acc: 66.7\n",
      "Epoch: [9/10], Step: [1201/3125], Val_Acc: 66.2\n",
      "Epoch: [9/10], Step: [1501/3125], Val_Acc: 65.9\n",
      "Epoch: [9/10], Step: [1801/3125], Val_Acc: 65.6\n",
      "Epoch: [9/10], Step: [2101/3125], Val_Acc: 67.2\n",
      "Epoch: [9/10], Step: [2401/3125], Val_Acc: 66.1\n",
      "Epoch: [9/10], Step: [2701/3125], Val_Acc: 67.2\n",
      "Epoch: [9/10], Step: [3001/3125], Val_Acc: 65.9\n",
      "Epoch: [10/10], Step: [301/3125], Val_Acc: 65.2\n",
      "Epoch: [10/10], Step: [601/3125], Val_Acc: 65.3\n",
      "Epoch: [10/10], Step: [901/3125], Val_Acc: 65.9\n",
      "Epoch: [10/10], Step: [1201/3125], Val_Acc: 66.0\n",
      "Epoch: [10/10], Step: [1501/3125], Val_Acc: 67.5\n",
      "Epoch: [10/10], Step: [1801/3125], Val_Acc: 66.9\n",
      "Epoch: [10/10], Step: [2101/3125], Val_Acc: 65.4\n",
      "Epoch: [10/10], Step: [2401/3125], Val_Acc: 66.2\n",
      "Epoch: [10/10], Step: [2701/3125], Val_Acc: 66.2\n",
      "Epoch: [10/10], Step: [3001/3125], Val_Acc: 66.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "val_acc_record = []\n",
    "#train_acc_record = []\n",
    "val_loss_record = []\n",
    "#train_loss_record = []\n",
    "step_record = []\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (s1, s2, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(s1.to(device), s2.to(device), lengths.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model2(val_loader, model)\n",
    "            val_acc_record.append(val_acc)\n",
    "            val_loss_record.append(val_loss)\n",
    "            #train_acc, train_loss = test_model2(train_loader, model)\n",
    "            #train_acc_record.append(train_acc)\n",
    "            #train_loss_record.append(train_loss)\n",
    "            step_record.append( i + epoch * total_step)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Val_Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            if val_acc > best_val_acc:\n",
    "                with open(model_name, 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                    best_val_loss = val_loss\n",
    "\n",
    "#training_curve = zip(step_record, train_acc_record, train_loss_record, val_acc_record, val_loss_record)\n",
    "#pkl.dump(training_curve, open(result_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_name, 'rb') as f:\n",
    "    best_model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader0 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  True label:  0   Predicted label:  2\n",
      "1  True label:  2   Predicted label:  2\n",
      "2  True label:  2   Predicted label:  2\n",
      "3  True label:  2   Predicted label:  2\n",
      "4  True label:  2   Predicted label:  2\n",
      "5  True label:  2   Predicted label:  0\n",
      "6  True label:  2   Predicted label:  2\n",
      "7  True label:  0   Predicted label:  1\n",
      "8  True label:  1   Predicted label:  1\n",
      "9  True label:  0   Predicted label:  0\n",
      "10  True label:  2   Predicted label:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml5893/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ml5893/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "for s1, s2, lengths, labels in val_loader0:\n",
    "    s1_batch, s2_batch, lengths_batch, label_batch = s1.to(device), s2.to(device), lengths.to(device), labels.to(device)\n",
    "    outputs = F.softmax(best_model(s1_batch, s2_batch, lengths_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    print(n, \" True label: \", labels.data.numpy()[0], \"  Predicted label: \", (predicted.data).to('cpu').numpy()[0][0])\n",
    "    n = n+1\n",
    "    if n>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrect Exmaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Three women on a stage , one wearing red shoes , black pants , and a gray shirt is sitting on a prop , another is sitting on the floor , and the third wearing a black shirt and pants is standing , as a gentleman in the back tunes an instrument .',\n",
       " 'There are two women standing on the stage',\n",
       " 'contradiction',\n",
       " 'False Prediction: Entailment')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[0], data_val_s2[0], data_val_label[0], 'False Prediction: Entailment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two people are in a green forest .',\n",
       " 'The forest is not dead .',\n",
       " 'entailment',\n",
       " 'False Prediciton: Contradiction')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[5], data_val_s2[5], data_val_label[5], 'False Prediciton: Contradiction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two women , one walking her dog the other pushing a stroller .',\n",
       " 'There is a snowstorm .',\n",
       " 'contradiction',\n",
       " 'False Prediciton: Neutral')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[7], data_val_s2[7], data_val_label[7], 'False Prediciton: Neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Four people sit on a subway two read books , one looks at a cellphone and is wearing knee high boots .',\n",
       " 'Multiple people are on a subway together , with each of them doing their own thing .',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[1], data_val_s2[1], data_val_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A group of numbered participants walk down the street together .',\n",
       " 'Participants wait for the beginning of the walkathon .',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[8], data_val_s2[8], data_val_label[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Three people and a white dog are sitting in the sand on a beach .',\n",
       " 'Three dogs and a person are sitting in the snow .',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val_s1[9], data_val_s2[9], data_val_label[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on MultiMLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open('mnli_val.tsv','r', encoding = 'utf-8', newline = '') as file:\n",
    "    tsvreader = csv.reader(file, delimiter='\\t')\n",
    "    data_test = [line for line in tsvreader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence1', 'sentence2', 'label', 'genre']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_head = data_test[0]\n",
    "data_test_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = data_test[1:]\n",
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_test, s2_test, label_test, genre = zip(*data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fiction', 'government', 'slate', 'travel', 'telephone']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_set = list(set(genre))\n",
    "genre_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = {'slate': [], 'travel': [], 'fiction': [], 'government': [], 'telephone': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_test)):\n",
    "    data_t[data_test[i][-1]].append(data_test[i][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml5893/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ml5893/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.11557788944724\n",
      "government\n",
      "44.19291338582677\n",
      "slate\n",
      "43.11377245508982\n",
      "travel\n",
      "44.09368635437882\n",
      "telephone\n",
      "47.56218905472637\n"
     ]
    }
   ],
   "source": [
    "for gen in genre_set:\n",
    "    print(gen)\n",
    "    \n",
    "    data_s1, data_s2, data_label = zip(*data_t[gen])\n",
    "    data_s1_tokens, _ = tokenize_dataset_ngrams(data_s1)\n",
    "    data_s2_tokens, _ = tokenize_dataset_ngrams(data_s2)\n",
    "    data_label0 = convert_labels(data_label)\n",
    "    \n",
    "    data_s1_indices = token2index_dataset(data_s1_tokens, token2id)\n",
    "    data_s2_indices = token2index_dataset(data_s2_tokens, token2id)\n",
    "    \n",
    "    test_dataset = SNLIDataset(data_s1_indices, data_s2_indices, data_label0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "    test_acc, test_loss = test_model2(test_loader, best_model)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b576fd03c18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(step_record, train_acc_record, label = 'train accuracy')\n",
    "plt.plot(step_record, val_acc_record, label = 'train accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs Training Step\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.savefig(\"result/rnn_acc_hs200_ls200.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply the two presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, pretrained_weight):\n",
    "        \"\"\"\n",
    "        GRU accepts the following hyperparams:\n",
    "        input_size - The number of expected features of in the input x\n",
    "        hidden_size - The number of features in the hidden state h\n",
    "        num_layers - Here we use the Default:1\n",
    "        bias - True\n",
    "        batch_first - The input and output tensors are provided as (batch, seq, feature)\n",
    "        bidirectional - \n",
    "        ===================================================================================\n",
    "        Note: padding_idx = 0\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        layer_size2 = 20\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx = 0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional = True) # The first dimension is the batch dimension\n",
    "        self.linear1 = nn.Linear(2*hidden_size, layer_size2)\n",
    "        self.linear2 = nn.Linear(layer_size2, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden.to(device)\n",
    "\n",
    "    def forward(self, s1, s2, lengths):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len = s1.size()\n",
    "\n",
    "        self.hidden_s1 = self.init_hidden(batch_size)\n",
    "        self.hidden_s2 = self.init_hidden(batch_size)\n",
    "        \n",
    "        embed_s1 = self.embedding(s1)\n",
    "        embed_s2 = self.embedding(s2)\n",
    "    \n",
    "        rnn_out_s1, self.hidden_s1 = self.gru(embed_s1, self.hidden_s1)\n",
    "        rnn_out_s2, self.hidden_s2 = self.gru(embed_s2, self.hidden_s2)\n",
    "        \n",
    "        rnn_out_s1 = torch.sum(rnn_out_s1, dim=1)\n",
    "        rnn_out_s2 = torch.sum(rnn_out_s2, dim=1)\n",
    "        \n",
    "        rnn_out = rnn_out_s1 * rnn_out_s2\n",
    "        \n",
    "        rnn_out = self.linear1(rnn_out)\n",
    "        rnn_out = F.relu(rnn_out)\n",
    "        logits = self.linear2(rnn_out)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token), pretrained_weight = pre_emb_matrix).to(device)\n",
    "result_FILE = 'result/rnn_hs200_add.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6484783\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [301/3125], Val_Acc: 33.1, Train_Acc: 33.16, Val_loss: 1.1035100929439068, Train_loss: 1.103185442085266\n",
      "Epoch: [1/10], Step: [601/3125], Val_Acc: 33.1, Train_Acc: 33.164, Val_loss: 1.1006201580166817, Train_loss: 1.1011054068374633\n",
      "Epoch: [1/10], Step: [901/3125], Val_Acc: 33.1, Train_Acc: 33.182, Val_loss: 1.0997655615210533, Train_loss: 1.0999165306091307\n",
      "Epoch: [1/10], Step: [1201/3125], Val_Acc: 33.1, Train_Acc: 33.167, Val_loss: 1.0998134687542915, Train_loss: 1.0994847689819336\n",
      "Epoch: [1/10], Step: [1501/3125], Val_Acc: 33.1, Train_Acc: 33.163, Val_loss: 1.09901949390769, Train_loss: 1.0991282250213623\n",
      "Epoch: [1/10], Step: [1801/3125], Val_Acc: 33.1, Train_Acc: 33.17, Val_loss: 1.0990152768790722, Train_loss: 1.0987879649734498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-94e7654f6bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyenv/py3.6.3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "val_acc_record = []\n",
    "train_acc_record = []\n",
    "val_loss_record = []\n",
    "train_loss_record = []\n",
    "step_record = []\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (s1, s2, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(s1.to(device), s2.to(device), lengths.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 300 == 0:\n",
    "            # validate\n",
    "            val_acc, val_loss = test_model2(val_loader, model)\n",
    "            val_acc_record.append(val_acc)\n",
    "            val_loss_record.append(val_loss)\n",
    "            train_acc, train_loss = test_model2(train_loader, model)\n",
    "            train_acc_record.append(train_acc)\n",
    "            train_loss_record.append(train_loss)\n",
    "            step_record.append( i + epoch * total_step)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Val_Acc: {}, Train_Acc: {}, Val_loss: {}, Train_loss: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc, val_loss, train_loss))\n",
    "\n",
    "training_curve = zip(step_record, train_acc_record, train_loss_record, val_acc_record, val_loss_record)\n",
    "pkl.dump(training_curve, open(result_FILE, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
